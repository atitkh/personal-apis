const memoryService = require('./memoryService');
const llmService = require('./llmService');
const memoryIntelligence = require('./memoryIntelligenceService');
const mcpService = require('./mcpService');
const { logger } = require('../utils/logger');
const crypto = require('crypto');

class VortexService {
  constructor() {
    this.personality = {
      name: "Vortex",
      role: "friend",
      description: "created and developed by Atit Kharel in 2025, a obedient and polite friend, who happens to remember everything and knows a lot about tech",
      traits: [
        "Casual and genuine",
        "Gets straight to the point",
        "Remembers past conversations naturally",
        "Knows tech stuff but doesn't show off",
        "Talks like a real person, not a robot",
        "Helpful and friendly but professional not arrogant"
      ]
    };
  }

  /**
   * Process a chat message through the full Vortex pipeline
   */
  async processChat({ userId, message, conversationId, context = {}, debug = false }) {
    try {
      // Generate conversation ID if not provided
      if (!conversationId) {
        conversationId = `conv_${Date.now()}_${crypto.randomUUID().slice(0, 8)}`;
      }

      // Validate message content
      const messageContent = message?.trim() || '';
      if (!messageContent) {
        logger.warn('Empty message received in processChat', { userId, conversationId, source: context.source });
      }

      logger.debug('Processing chat message', {
        userId,
        conversationId,
        messageLength: messageContent.length,
        messagePreview: messageContent.substring(0, 100),
        source: context.source
      });

      // STEP 1: UNIFIED MESSAGE ANALYSIS (replaces separate evaluation + query enhancement + MCP check)
      // This single LLM call provides: importance evaluation, search queries, and MCP intent
      let messageAnalysis = null;
      let storageDecisions = []; // Track what we stored for debugging
      
      if (messageContent) {
        try {
          messageAnalysis = await memoryIntelligence.analyzeMessage({
            message: messageContent,
            role: 'user',
            context: {
              previousMessage: context.lastAssistantMessage || null,
              conversationSummary: context.conversationSummary || null
            }
          });

          logger.debug('Unified message analysis', {
            importance: messageAnalysis.evaluation.importance,
            category: messageAnalysis.evaluation.category,
            shouldStore: messageAnalysis.evaluation.shouldStore,
            queriesGenerated: messageAnalysis.retrieval.queries.length,
            needsMCP: messageAnalysis.mcpIntent.needsTools
          });
        } catch (analysisError) {
          logger.warn('Message analysis failed, using defaults', { error: analysisError.message });
          messageAnalysis = memoryIntelligence.getDefaultAnalysis(messageContent);
        }
      }

      const userMessageEvaluation = messageAnalysis?.evaluation || { 
        shouldStore: true, 
        storageType: 'conversations', 
        importance: 5, 
        category: 'context' 
      };

      // STEP 2: Retrieve relevant context using analysis-generated queries
      let relevantMemories = [];
      let queryEnhancementDebug = null;
      try {
        // Use the 2 queries generated by unified analysis
        const searchQueries = messageAnalysis?.retrieval?.queries || [messageContent];

        // Store for debug output
        queryEnhancementDebug = {
          originalQuery: messageContent,
          enhancedQueries: searchQueries,
          keywords: messageAnalysis?.retrieval?.keywords || [],
          categories: messageAnalysis?.retrieval?.categories || []
        };

        logger.debug('Using analysis-generated queries for retrieval', {
          original: messageContent?.substring(0, 50),
          queries: searchQueries
        });

        // Retrieve memories using the 2 generated queries
        const allMemories = [];
        for (const query of searchQueries.slice(0, 2)) { // Use only 2 queries
          const memories = await memoryService.getRelevantContext({
            userId,
            query,
            conversationId,
            limit: 20
          });
          allMemories.push(...memories);
        }

        // SMART RELEVANCE FILTER - Hybrid adaptive approach
        const relevantOnly = this.smartFilterMemories(allMemories);

        // Console log for debugging
        console.log('\nðŸ” SMART RELEVANCE FILTER:');
        console.log(`  Retrieved: ${allMemories.length} memories`);
        console.log(`  After smart filter: ${relevantOnly.length} memories`);
        console.log(`  Filtered out: ${allMemories.length - relevantOnly.length} irrelevant memories`);
        if (allMemories.length > 0) {
          const distances = allMemories.map(m => m.distance || 1);
          console.log('  Distance range:', {
            min: Math.min(...distances).toFixed(3),
            max: Math.max(...distances).toFixed(3),
            kept_max: relevantOnly.length > 0 ? Math.max(...relevantOnly.map(m => m.distance || 1)).toFixed(3) : 'N/A'
          });
        }

        logger.debug('Smart relevance filtering', {
          before: allMemories.length,
          after: relevantOnly.length,
          filtered: allMemories.length - relevantOnly.length
        });

        // Deduplicate by content
        const seenContent = new Set();
        const uniqueMemories = relevantOnly.filter(m => {
          const key = (m.content || m.document || '').substring(0, 100);
          if (seenContent.has(key)) return false;
          seenContent.add(key);
          return true;
        });

        // Re-rank memories for relevance (only if >12 memories - increased threshold)
        if (uniqueMemories.length > 12) {
          relevantMemories = await memoryIntelligence.rerankMemories(
            messageContent,
            uniqueMemories,
            12
          );
        } else {
          relevantMemories = uniqueMemories;
        }

        // Sort memories chronologically (oldest first)
        relevantMemories.sort((a, b) => {
          const timeA = new Date(a.metadata?.timestamp || 0);
          const timeB = new Date(b.metadata?.timestamp || 0);
          return timeA - timeB;
        });

        logger.debug('Smart retrieval complete', {
          totalRetrieved: allMemories.length,
          afterDedup: uniqueMemories.length,
          afterRerank: relevantMemories.length
        });

      } catch (retrievalError) {
        logger.warn('Smart retrieval failed, using basic retrieval', { error: retrievalError.message });
        // Fallback to basic retrieval
        relevantMemories = await memoryService.getRelevantContext({
          userId,
          query: messageContent || 'general conversation',
          conversationId,
          limit: 10
        });
        // Apply relevance threshold even in fallback
        relevantMemories = relevantMemories.filter(m => (m.distance || 1) < 1.4);
      }

      // Build conversation context for LLM
      const conversationContext = await this.buildConversationContext({
        currentMessage: messageContent,
        conversationId,
        relevantMemories,
        userContext: context
      });

      // Build proper system prompt with memory context using llmService
      const systemPrompt = llmService.buildSystemPrompt({
        userContext: context.user || { name: 'Atit' },
        relevantMemories,
        currentTime: new Date().toISOString(),
        personality: this.personality
      });

      // Capture debug information if requested
      let debugInfo = null;
      if (debug) {
        debugInfo = {
          prompt: {
            system: systemPrompt,
            messages: conversationContext,
            message_count: conversationContext.length,
            total_chars: JSON.stringify(conversationContext).length
          },
          memory: {
            relevant_memories_count: relevantMemories.length,
            memories: relevantMemories.map(m => ({
              type: m.type || m.metadata?.type,
              content_preview: (m.content || m.document)?.substring(0, 100) + '...',
              distance: m.distance
            }))
          },
          unified_analysis: messageAnalysis ? {
            evaluation: {
              importance: messageAnalysis.evaluation.importance,
              category: messageAnalysis.evaluation.category,
              shouldStore: messageAnalysis.evaluation.shouldStore,
              storageType: messageAnalysis.evaluation.storageType,
              summary: messageAnalysis.evaluation.summary,
              keyFacts: messageAnalysis.evaluation.keyFacts,
              reasoning: messageAnalysis.evaluation.reasoning
            },
            retrieval: {
              originalQuery: messageContent,
              generatedQueries: messageAnalysis.retrieval.queries,
              keywords: messageAnalysis.retrieval.keywords,
              categories: messageAnalysis.retrieval.categories
            },
            mcpIntent: {
              needsTools: messageAnalysis.mcpIntent.needsTools,
              confidence: messageAnalysis.mcpIntent.confidence,
              likelyTools: messageAnalysis.mcpIntent.likelyTools,
              intentType: messageAnalysis.mcpIntent.intentType,
              reasoning: messageAnalysis.mcpIntent.reasoning
            }
          } : null,
          timestamp: new Date().toISOString()
        };
      }

      // STEP 3: Check if message requires tool/MCP execution (using unified analysis)
      let mcpResult = null;
      const needsMCP = messageAnalysis?.mcpIntent?.needsTools || false;
      
      if (needsMCP) {
        try {
          logger.debug('MCP execution needed per unified analysis', {
            confidence: messageAnalysis.mcpIntent.confidence,
            likelyTools: messageAnalysis.mcpIntent.likelyTools,
            intentType: messageAnalysis.mcpIntent.intentType
          });

          // Get raw recent conversation for MCP (not filtered/deduplicated like conversationContext)
          // This ensures we have the actual conversation flow for pronoun resolution
          let recentForMCP = [];
          try {
            const rawRecentMessages = await memoryService.getRecentConversation({
              userId,
              conversationId,
              limit: 10 // Last 10 messages (5 exchanges)
            });
            
            // Take last 8 messages, but exclude the very last one if it matches current message
            // (the current message is passed separately to MCP)
            const currentMsgNormalized = messageContent?.toLowerCase().trim();
            const lastMsg = rawRecentMessages[rawRecentMessages.length - 1];
            const lastMsgContent = (lastMsg?.document || lastMsg?.content || '').toLowerCase().trim();
            
            // If the last message matches current, exclude it (it's the current message already stored)
            const messagesToUse = lastMsgContent === currentMsgNormalized
              ? rawRecentMessages.slice(0, -1)  // Exclude last
              : rawRecentMessages;
            
            recentForMCP = messagesToUse
              .slice(-8) // Last 8 messages (4 exchanges)
              .map(msg => ({
                role: msg.metadata?.role === 'assistant' ? 'assistant' : 'user',
                content: msg.document || msg.content
              }));
          } catch (e) {
            logger.debug('Could not get recent messages for MCP', { error: e.message });
          }
          
          mcpResult = await mcpService.processMessage({
            message: messageContent,
            userId,
            conversationId,
            context,
            recentMessages: recentForMCP
          });

          // If tools were used (or attempted), inject results into conversation context
          if (mcpResult.needsTools) {
            // Check if any tools failed
            const failedTools = (mcpResult.toolResults || []).filter(r => !r.success);
            const successfulTools = (mcpResult.toolResults || []).filter(r => r.success);
            
            let toolContext = '';
            
            // Use the LLM's summary if available (it already parsed and understood the data)
            if (mcpResult.llmSummary) {
              toolContext = `TOOL DATA:\n${mcpResult.llmSummary}`;
            } else if (mcpResult.contextForLLM) {
              // Fallback to formatted results if no summary
              toolContext = `TOOL DATA:\n${mcpResult.contextForLLM}`;
            }
            
            // If tools failed, add explicit failure context so LLM acknowledges it
            if (failedTools.length > 0) {
              toolContext += '\n\nIMPORTANT: Some tool actions FAILED. You MUST acknowledge this failure to the user.';
              toolContext += '\nFailed actions:';
              failedTools.forEach(f => {
                toolContext += `\n- ${f.tool}: ${f.error || 'Unknown error'}`;
              });
              toolContext += '\n\nDo NOT say you cannot do the action. Say the action was ATTEMPTED but encountered an error.';
            }
            
            if (toolContext) {
              conversationContext.push({
                role: 'system',
                content: toolContext
              });
              
              // Debug: Log what we're actually injecting
              console.log('\nðŸ”§ TOOL CONTEXT INJECTED INTO CONVERSATION:');
              console.log('Role: system');
              console.log('Content:', toolContext);
              console.log('===============================================\n');
            }
            
            logger.debug('MCP tools executed', {
              toolsUsed: mcpResult.toolsUsed,
              successCount: successfulTools.length,
              failCount: failedTools.length
            });
          }

          // Add full MCP response to debug output - include RAW response
          if (debug && debugInfo) {
            debugInfo.mcp = mcpResult ? {
              ...mcpResult,  // Spread entire response
              _raw: JSON.stringify(mcpResult)  // Also include stringified version
            } : { error: 'mcpResult was null/undefined' };
          }
        } catch (mcpError) {
          logger.error('MCP processing failed', { error: mcpError.message });
          // Continue without tools - don't break the chat
          if (debug && debugInfo) {
            debugInfo.mcp = { error: mcpError.message, stack: mcpError.stack };
          }
        }
      } else {
        logger.debug('Skipping MCP - unified analysis determined no tools needed', {
          confidence: messageAnalysis?.mcpIntent?.confidence || 0,
          intentType: messageAnalysis?.mcpIntent?.intentType || 'query'
        });
        if (debug && debugInfo) {
          debugInfo.mcp = { 
            skipped: true, 
            reason: 'unified_analysis_no_tools',
            analysis: messageAnalysis?.mcpIntent 
          };
        }
      }

      // Get LLM response with focused parameters
      console.log('\nðŸ“¨ SENDING TO MAIN LLM:');
      console.log('Conversation messages:', conversationContext.length);
      conversationContext.forEach((msg, i) => {
        console.log(`  [${i}] ${msg.role}: ${msg.content?.substring(0, 100)}${msg.content?.length > 100 ? '...' : ''}`);
      });
      console.log('===============================================\n');
      
      const llmResponse = await llmService.generateResponse({
        messages: conversationContext,
        userId,
        systemPrompt,
        temperature: 0.3, // Lower temperature for more focused responses
        maxTokens: 500    // Shorter response length to encourage conciseness
      });

      // STEP 4: NOW store messages (after LLM response, so retrieval wasn't polluted)
      // Store user message based on evaluation
      if (messageContent && userMessageEvaluation) {
        const { importance, category, keyFacts, summary, storageType } = userMessageEvaluation;
        
        // HIGH IMPORTANCE (7-10): Store key facts appropriately + full message
        if (importance >= 7) {
          // Store extracted key facts based on category
          if (keyFacts && keyFacts.length > 0) {
            for (const fact of keyFacts) {
              let result;
              
              // Route facts to the appropriate collection
              if (['fact', 'relationship'].includes(category) || storageType === 'facts') {
                // Store as fact (name, location, relationships, etc.)
                result = await memoryService.storeFact({
                  userId,
                  fact: fact,
                  category: category,
                  context: { 
                    ...context, 
                    importance,
                    extractedFrom: 'conversation' 
                  }
                });
                if (result.skipped) {
                  storageDecisions.push({ type: 'fact', content: fact.substring(0, 30), importance, skipped: true, reason: 'duplicate' });
                } else {
                  storageDecisions.push({ type: 'fact', content: fact.substring(0, 30), importance, stored: true });
                }
              } else {
                // Store as preference (likes, dislikes, instructions)
                result = await memoryService.storePreference({
                  userId,
                  category: category,
                  preference: fact,
                  context: { 
                    ...context, 
                    importance,
                    extractedFrom: 'conversation' 
                  }
                });
                if (result.skipped) {
                  storageDecisions.push({ type: 'preference', content: fact.substring(0, 30), importance, skipped: true, reason: 'duplicate' });
                } else {
                  storageDecisions.push({ type: 'preference', content: fact.substring(0, 30), importance, stored: true });
                }
              }
            }
          }
          
          // Store in events if it's an event/task category
          if (['event', 'task', 'reminder'].includes(category)) {
            const result = await memoryService.storeEvent({
              userId,
              eventType: category,
              domain: 'user_stated',
              userIntent: summary || messageContent,
              systemResponse: llmResponse.content,
              context: { ...context, importance, category }
            });
            if (result.skipped) {
              storageDecisions.push({ type: 'event', category, importance, skipped: true, reason: 'duplicate' });
            } else {
              storageDecisions.push({ type: 'event', category, importance, stored: true });
            }
          }
          
          // Store in conversations - use summary as content if available, keep raw for reference
          const convResult = await memoryService.storeConversation({
            userId,
            conversationId,
            role: 'user',
            content: summary || messageContent,
            context: {
              ...context,
              importance,
              category,
              ...(summary && summary !== messageContent ? { rawMessage: messageContent } : {})
            }
          });
          if (convResult.skipped) {
            storageDecisions.push({ type: 'conversation', importance, skipped: true, reason: 'duplicate' });
          } else {
            storageDecisions.push({ type: 'conversation', importance, stored: true, hasSummary: !!summary });
          }
        }
        // MEDIUM IMPORTANCE (4-6): Store in conversations
        else if (importance >= 4) {
          const result = await memoryService.storeConversation({
            userId,
            conversationId,
            role: 'user',
            content: summary || messageContent,
            context: {
              ...context,
              importance,
              category,
              ...(summary && summary !== messageContent ? { rawMessage: messageContent } : {})
            }
          });
          if (result.skipped) {
            storageDecisions.push({ type: 'conversation', importance, skipped: true, reason: 'duplicate' });
          } else {
            storageDecisions.push({ type: 'conversation', importance, stored: true, hasSummary: !!summary });
          }
        }
        // LOW IMPORTANCE (1-3): Still store in conversations for working memory continuity
        // This ensures the AI can follow the conversation flow even for casual messages
        else {
          const result = await memoryService.storeConversation({
            userId,
            conversationId,
            role: 'user',
            content: messageContent, // Store original, no summary needed for simple messages
            context: {
              ...context,
              importance,
              category,
              isLowPriority: true // Mark as low priority for potential cleanup later
            }
          });
          if (result.skipped) {
            storageDecisions.push({ type: 'conversation', importance, skipped: true, reason: 'duplicate' });
          } else {
            storageDecisions.push({ type: 'conversation', importance, stored: true, note: 'working memory only' });
          }
          logger.debug('Stored low importance message for working memory', { importance, category });
        }
        
        // Log storage decisions
        console.log('\nðŸ“¦ STORAGE DECISIONS:', JSON.stringify(storageDecisions, null, 2));
      }

      // Store assistant response
      await memoryService.storeConversation({
        userId,
        conversationId,
        role: 'assistant',
        content: llmResponse.content,
        context: {
          ...context,
          model_used: llmResponse.model,
          tokens_used: llmResponse.usage?.total_tokens
        }
      });

      // Use explicit action from evaluation (merged - no separate LLM call needed)
      // BUT skip if MCP already handled tools - avoid duplicate action handling
      let detectedActions = [];
      if (mcpResult?.needsTools && mcpResult?.toolsUsed?.length > 0) {
        // MCP already handled the action - convert tool results to action format for tracking
        detectedActions = mcpResult.toolsUsed.map(tool => ({
          type: 'tool_executed',
          tool: tool,
          handled_by: 'mcp',
          results: mcpResult.toolResults?.find(r => r.tool === tool)
        }));
        logger.debug('Actions handled by MCP, skipping explicit action detection', { toolsUsed: mcpResult.toolsUsed });
      } else if (userMessageEvaluation?.explicitAction) {
        // No MCP tools used, check for explicit memory actions (remember/remind/note)
        detectedActions = [userMessageEvaluation.explicitAction];
        // Process detected actions
        for (const action of detectedActions) {
          await this.handleDetectedAction(action, llmResponse.content, userId, conversationId, context);
        }
      }

      // Add LLM response to debug info if debug mode is enabled
      if (debug && debugInfo) {
        debugInfo.llm_request = {
          system_prompt: systemPrompt,
          messages: conversationContext.map(m => ({
            role: m.role,
            content: m.content
          })),
          temperature: 0.3,
          max_tokens: 500
        };
        debugInfo.llm_response = {
          model: llmResponse.model,
          tokens_used: llmResponse.usage?.total_tokens,
          response_chars: llmResponse.content.length,
          full_response: llmResponse.content,
          actions_detected: detectedActions.length,
          actions: detectedActions.map(a => ({ type: a.type, content: a.content?.substring(0, 50) + '...' }))
        };
      }

      const result = {
        response: llmResponse.content,
        conversation_id: conversationId,
        actions: detectedActions,
        metadata: {
          tokens_used: llmResponse.usage?.total_tokens,
          model: llmResponse.model,
          memories_retrieved: relevantMemories.length,
          actions_detected: detectedActions.length,
          timestamp: new Date().toISOString()
        },
        // Always include memory intelligence summary for browser console debugging
        memory_intelligence: {
          user_evaluation: userMessageEvaluation ? {
            importance: userMessageEvaluation.importance,
            category: userMessageEvaluation.category,
            shouldStore: userMessageEvaluation.shouldStore,
            storageType: userMessageEvaluation.storageType,
            keyFacts: userMessageEvaluation.keyFacts || [],
            summary: userMessageEvaluation.summary || null,
            reasoning: userMessageEvaluation.reasoning || null
          } : null,
          storage_decisions: storageDecisions,
          query_enhancement: queryEnhancementDebug ? {
            queries_used: queryEnhancementDebug.enhancedQueries?.length || 1,
            queries: queryEnhancementDebug.enhancedQueries || [],
            categories: queryEnhancementDebug.categories || []
          } : null,
          // Include unified analysis data even when debug=false
          unified_analysis: messageAnalysis ? {
            evaluation: {
              importance: messageAnalysis.evaluation.importance,
              category: messageAnalysis.evaluation.category,
              shouldStore: messageAnalysis.evaluation.shouldStore,
              storageType: messageAnalysis.evaluation.storageType
            },
            retrieval: {
              queries: messageAnalysis.retrieval.queries,
              categories: messageAnalysis.retrieval.categories
            },
            mcpIntent: {
              needsTools: messageAnalysis.mcpIntent.needsTools,
              confidence: messageAnalysis.mcpIntent.confidence,
              intentType: messageAnalysis.mcpIntent.intentType
            }
          } : null
        }
      };

      // Include debug info if requested
      if (debug && debugInfo) {
        result.debug = debugInfo;
      }

      return result;

    } catch (error) {
      logger.error('Vortex chat processing error', { 
        userId, 
        conversationId, 
        error: error.message 
      });
      throw error;
    }
  }



  /**
   * Build conversation context for LLM
   */
  async buildConversationContext({ currentMessage, conversationId, relevantMemories, userContext }) {
    const context = [];

    // Don't add memories here - they're handled in the system prompt via llmService.buildSystemPrompt
    // This prevents duplication of memory context

    // Get recent conversation turns from this session
    try {
      const recentMessages = await memoryService.getRecentConversation({
        userId: userContext.user?._id?.toString() || userContext.userId || userContext.user,
        conversationId,
        limit: 15 // Working memory: industry standard 10-15 messages for conversation context
      });
      
      // Filter and deduplicate - exclude messages that match current message
      const seenContent = new Set();
      const currentMsgNormalized = currentMessage?.toLowerCase().trim();
      
      if (recentMessages.length > 0) {
        // Take last 10 messages (5 exchanges) after filtering - industry standard working memory
        const filteredMessages = recentMessages.filter(msg => {
          const content = (msg.document || msg.content || '').toLowerCase().trim();
          
          // Skip if it's the same as current message (we'll add it at the end)
          if (content === currentMsgNormalized) {
            return false;
          }
          
          // Skip if we've already seen very similar content
          if (seenContent.has(content)) {
            return false;
          }
          
          // Skip if content is too similar to something we've seen (>80% word overlap)
          for (const seen of seenContent) {
            if (this.textSimilarity(content, seen) > 0.8) {
              return false;
            }
          }
          
          seenContent.add(content);
          return true;
        }).slice(-10); // Keep last 10 unique messages (5 exchanges)
        
        filteredMessages.forEach(msg => {
          const role = msg.metadata?.role || 'user';
          context.push({
            role: role === 'assistant' ? 'assistant' : 'user',
            content: msg.document || msg.content
          });
        });
      }
    } catch (error) {
      // If recent messages retrieval fails, continue without them
      console.warn('Could not retrieve recent messages:', error.message);
    }

    // Add current message at the end (this is the new message being processed)
    context.push({
      role: 'user',
      content: currentMessage
    });

    return context;
  }

  /**
   * Simple text similarity check (Jaccard similarity on words)
   */
  textSimilarity(text1, text2) {
    const words1 = new Set(text1.toLowerCase().split(/\s+/).filter(w => w.length > 2));
    const words2 = new Set(text2.toLowerCase().split(/\s+/).filter(w => w.length > 2));
    
    const intersection = new Set([...words1].filter(x => words2.has(x)));
    const union = new Set([...words1, ...words2]);
    
    return union.size > 0 ? intersection.size / union.size : 0;
  }

  /**
   * Handle detected action commands using LLM classification
   */
  async handleDetectedAction(action, assistantResponse, userId, conversationId, context) {
    try {
      const { type, content, confidence } = action;
      
      logger.info('Processing detected action', {
        userId,
        actionType: type,
        content,
        confidence
      });

      switch (type) {
        case 'remember':
          // Store as preference or important information
          await memoryService.storeEvent({
            userId,
            eventType: 'information_storage',
            domain: 'preferences',
            userIntent: `Remember: ${content}`,
            systemResponse: assistantResponse,
            context: {
              ...context,
              action_type: 'remember',
              confidence
            }
          });
          break;

        case 'remind':
          // Store as reminder/note
          await memoryService.storeEvent({
            userId,
            eventType: 'reminder_set',
            domain: 'productivity',
            userIntent: `Remind: ${content}`,
            systemResponse: assistantResponse,
            context: {
              ...context,
              action_type: 'remind',
              confidence
            }
          });
          break;

        case 'search':
          // Log search request for learning patterns
          await memoryService.storeEvent({
            userId,
            eventType: 'memory_search',
            domain: 'information_retrieval',
            userIntent: `Search: ${content}`,
            systemResponse: assistantResponse,
            context: {
              ...context,
              action_type: 'search',
              confidence
            }
          });
          break;

        case 'note':
          // Store general note
          await memoryService.storeEvent({
            userId,
            eventType: 'note_creation',
            domain: 'general',
            userIntent: `Note: ${content}`,
            systemResponse: assistantResponse,
            context: {
              ...context,
              action_type: 'note',
              confidence
            }
          });
          break;

        default:
          logger.warn('Unknown action type detected', { type, content });
      }

    } catch (error) {
      logger.error('Action handling error', {
        userId,
        action,
        error: error.message
      });
    }
  }

  /**
   * Get system status
   */
  async getSystemStatus() {
    try {
      const memoryStatus = await memoryService.getStatus();
      const llmStatus = await llmService.getStatus();
      
      // Try to get voice service status (optional)
      let voiceStatus = null;
      try {
        const voiceService = require('./voiceService');
        voiceStatus = await voiceService.getStatus();
      } catch (voiceError) {
        // Voice service is optional, so don't fail the entire status check
        voiceStatus = { overall: 'not_available', error: voiceError.message };
      }

      return {
        vortex: {
          status: 'operational',
          version: '1.0.0',
          personality: this.personality.name
        },
        memory: memoryStatus,
        llm: llmStatus,
        voice: voiceStatus,
        timestamp: new Date().toISOString()
      };

    } catch (error) {
      logger.error('Status check error', { error: error.message });
      return {
        vortex: {
          status: 'error',
          error: error.message
        },
        timestamp: new Date().toISOString()
      };
    }
  }

  /**
   * Get personality configuration (exposed for testing)
   */
  getPersonality() {
    return this.personality;
  }

  /**
   * Summarize a conversation and store key facts
   * Call this when a conversation ends or periodically for long conversations
   */
  async summarizeAndCompactConversation({ userId, conversationId, forceRegenerate = false }) {
    try {
      logger.info('Starting conversation summarization', { userId, conversationId });

      // Get all messages from this conversation
      const conversationMessages = await memoryService.getRecentConversation({
        userId,
        conversationId,
        limit: 50 // Get last 50 messages for summarization
      });

      if (conversationMessages.length < 3) {
        logger.debug('Not enough messages to summarize', { count: conversationMessages.length });
        return { skipped: true, reason: 'Not enough messages' };
      }

      // Generate conversation summary
      const summary = await memoryIntelligence.summarizeConversation(
        conversationMessages,
        { userId, conversationId }
      );

      logger.debug('Generated conversation summary', {
        factCount: summary.facts?.length || 0,
        preferenceCount: summary.preferences?.length || 0,
        taskCount: summary.tasks?.length || 0,
        topics: summary.topics
      });

      // Store extracted facts as preferences
      if (summary.facts && summary.facts.length > 0) {
        for (const fact of summary.facts) {
          await memoryService.storePreference({
            userId,
            category: 'fact',
            preference: fact,
            context: {
              source: 'summarization',
              conversationId,
              importance: 8
            }
          });
        }
      }

      // Store preferences
      if (summary.preferences && summary.preferences.length > 0) {
        for (const pref of summary.preferences) {
          await memoryService.storePreference({
            userId,
            category: 'preference',
            preference: pref,
            context: {
              source: 'summarization',
              conversationId,
              importance: 7
            }
          });
        }
      }

      // Store tasks as events
      if (summary.tasks && summary.tasks.length > 0) {
        for (const task of summary.tasks) {
          await memoryService.storeEvent({
            userId,
            eventType: 'task_identified',
            domain: 'productivity',
            userIntent: task,
            systemResponse: 'Task extracted from conversation',
            context: {
              source: 'summarization',
              conversationId
            }
          });
        }
      }

      // Store the overall summary as an event
      if (summary.summary) {
        await memoryService.storeEvent({
          userId,
          eventType: 'conversation_summary',
          domain: 'meta',
          userIntent: `Conversation about: ${summary.topics?.join(', ') || 'various topics'}`,
          systemResponse: summary.summary,
          context: {
            source: 'summarization',
            conversationId,
            sentiment: summary.sentiment,
            messageCount: conversationMessages.length
          }
        });
      }

      logger.info('Conversation summarization complete', {
        userId,
        conversationId,
        factsStored: summary.facts?.length || 0,
        preferencesStored: summary.preferences?.length || 0,
        tasksStored: summary.tasks?.length || 0
      });

      return {
        success: true,
        summary: summary.summary,
        facts: summary.facts,
        preferences: summary.preferences,
        tasks: summary.tasks,
        topics: summary.topics,
        sentiment: summary.sentiment
      };

    } catch (error) {
      logger.error('Conversation summarization failed', {
        userId,
        conversationId,
        error: error.message
      });
      return { success: false, error: error.message };
    }
  }

  /**
   * Get memory intelligence status and statistics
   */
  async getMemoryIntelligenceStatus() {
    return {
      enabled: true,
      thresholds: memoryIntelligence.thresholds,
      categories: Object.values(memoryIntelligence.categories),
      features: {
        smartStorage: true,
        queryEnhancement: true,
        reranking: true,
        summarization: true
      }
    };
  }

  /**
   * Smart memory filtering with hybrid adaptive approach
   * Combines static thresholds with dynamic adaptation based on result quality
   * 
   * Strategy:
   * 1. Always include top results if under strict quality threshold
   * 2. Dynamically adapt based on distance range (min/max)
   * 3. Stop at relevance cliffs (large gaps in distance)
   * 4. Enforce min/max result limits
   */
  smartFilterMemories(memories) {
    if (!memories || memories.length === 0) return [];

    // Sort by distance (lower = more relevant)
    const sorted = [...memories].sort((a, b) => (a.distance || 1) - (b.distance || 1));
    
    const distances = sorted.map(m => m.distance || 1);
    const min = distances[0];
    const max = distances[distances.length - 1];

    // Configuration
    const config = {
      strictThreshold: 1.3,      // Always include if distance < this
      looseThreshold: 1.8,       // Never include if distance > this
      minResults: 3,             // Try to return at least this many
      maxResults: 20,            // Never return more than this
      dynamicPercentile: 0.4,    // Keep within 40% of distance range
      gapMultiplier: 1.3         // Stop if next result is 30%+ farther (relevance cliff)
    };

    // Calculate dynamic threshold based on actual result distribution
    const dynamicThreshold = min + (max - min) * config.dynamicPercentile;
    const effectiveThreshold = Math.max(
      Math.min(config.strictThreshold, dynamicThreshold),
      min * 1.2  // At minimum, allow 20% above best result
    );

    const filtered = [];
    let prevDistance = 0;

    for (const memory of sorted) {
      const distance = memory.distance || 1;

      // Always try to include minimum results if under strict threshold
      if (filtered.length < config.minResults && distance < config.strictThreshold) {
        filtered.push(memory);
        prevDistance = distance;
        continue;
      }

      // Stop if we hit max results
      if (filtered.length >= config.maxResults) break;

      // Stop if beyond loose threshold (never include very poor matches)
      if (distance > config.looseThreshold) break;

      // Stop if beyond effective threshold
      if (distance > effectiveThreshold) break;

      // Gap detection: Stop if there's a relevance cliff
      // (next result is significantly farther than previous)
      if (prevDistance > 0 && distance / prevDistance > config.gapMultiplier) {
        // Only stop at gap if we have minimum results
        if (filtered.length >= config.minResults) break;
      }

      filtered.push(memory);
      prevDistance = distance;
    }

    logger.debug('Smart filter applied', {
      input: memories.length,
      output: filtered.length,
      minDistance: min.toFixed(3),
      maxDistance: max.toFixed(3),
      effectiveThreshold: effectiveThreshold.toFixed(3),
      dynamicThreshold: dynamicThreshold.toFixed(3)
    });

    return filtered;
  }
}

module.exports = new VortexService();