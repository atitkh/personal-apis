*September 15, 2020*

***

# Learn Languages: *Interactive Learning with AR*

<a href="https://lens.snapchat.com/3d32ee16732642519634841a3d373657" target="_blank"><img src="https://img.shields.io/badge/Snapchat-Lens-FFFC00" alt="ASL Learning"></a>

</br>
<img src="https://media2.giphy.com/media/2NOe6cOkCbote0JVqN/giphy.gif?cid=790b76113bd251a580beb71e418a19614c4f0791a56de416&rid=giphy.gif&ct=g" alt="Learn Languages Demo" height="500"/>

## Overview

**Learn Languages**, is a collection of multiple projects which featured a set of lenses for learning 17 different languages. I have always aspired to create an experience where users can learn more about the objects around them. This concept was ideal for beginning with learning the names of objects in various languages. For me, learning a language is tough, and I believe that starting with basic common words is a good approach.

## Inspiration Behind the Project

Learning a new language can be challenging, and I wanted to make the process more accessible and engaging. I envisioned an experience where users could point their camera at objects around them and learn the names of those objects in a different language. This idea combines everyday interactions with language learning, making it practical and fun.

## Development Journey

### Built With

![Snap Lens Studio](https://img.shields.io/badge/Snap%20Lens%20Studio-FFFC00?style=for-the-badge&logo=snapchat&logoColor=black)
![JavaScript](https://img.shields.io/badge/JavaScript-F7DF1E?style=for-the-badge&logo=javascript&logoColor=black)
![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)

### Early Access to SnapML

I knew that achieving this would be possible through Machine Learning (ML) and Augmented Reality (AR), but I wasn't sure how to start. When Snapchat was about to launch **SnapML**, I was able to get early access to this new feature through a beta version of Lens Studio. It was an exciting opportunity to learn the basics of ML and experiment with SnapML's capabilities while creating something useful.

### Building the Language Learning Lens

For these lenses, I used a ML model trained on the [ImageNet](https://www.image-net.org/index.php) [dataset](https://www.kaggle.com/c/imagenet-object-localization-challenge/overview/description), which includes images of over 1,000 different objects. When someone points their camera at an object, the lens can recognize it and then show the name of that object in the language they were learning. Along with the names, the lens also provides phonetics to help users learn the proper pronunciation of the translated words.

In collaboration with [Snap Inc.](https://snapchat.com) and organizations like [First Languages Australia](https://www.firstlanguages.org.au/), I developed a set of lenses for learning **17 different languages**.

### Languages Worked On

- [Yugambeh](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=9feabdc607464a30a64acb7a824f595c&metadata=01)
- [Wiradjuri](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=94e3191662cf498b8a0f2a57c78f95fe&metadata=01)
- [Wakka Wakka](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=d8d74c5252ad437c879b75f9ffdfff85&metadata=01)
- [Yawuru](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=6c10855352b94ce6a2cba595fcf387cd&metadata=01)
- [Tamil](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=1c2bbe642d5047aa85c8a878ce59d2ec&metadata=01)
- [Punjabi](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=a8c17d11761f4433bc62ffb629aa78b4&metadata=01)
- [Telugu](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=c991697a3a6b4db386b578d90491eca5&metadata=01)
- [Bengali](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=a43f1d217a874fcba6b5b65e3f2ef355&metadata=01)
- [Oriya](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=a2deade36b3e436f9cf959b077745826&metadata=01)
- [Marathi](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=59ea1016774b4157b77783a1b03925cd&metadata=01)
- [Eng Nep Newa](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=21f130c02ed04d34b5a0ad17cb2a18f8&metadata=01)
- [Nepali](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=37d4cd80b71e4d008963c96e08543a1f&metadata=01)
- [Kannada](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=7b734bae68aa4484993aa973c08985d6&metadata=01)
- [Indonesian](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=40485eb1c6004f08b1147a4fd37b2e16&metadata=01)
- [Spanish](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=3d32ee16732642519634841a3d373657&metadata=01)
- [Chinese](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=ec93868501d14aa5a49a1d42240cebcf&metadata=01)
- [French](https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=22776b1578f94824bb1b8947f5ef2426&metadata=011)

## Features

### Real-Time Object Recognition

- **Object Detection**: It recognizes everyday objects using a trained ML model.
- **Phonetic Pronunciation**: Also displays phonetics to assist with correct pronunciation.

### User-Friendly Interface 
Creating a user-friendly UI/UX was essential. The operation mode is simple: just point the camera at an object and tap on screen to learn its name.

</br>

<img src="https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExYjV2bHA0NzE2ZGgzNmluYXY0NjM0emRybHowd24zbjdwenEwNHl5bCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/dOVT21pU4LP1enDI8d/giphy.gif" alt="Learn Yugambeh Demo" height="500"/>

## Challenges Faced

- **Learning ML Basics**: As a newcomer to ML, understanding how to train and integrate models was a significant learning curve.
- **Model Optimization**: Ensuring the ML model worked efficiently within the constraints of Lens Studio required optimization and conversion to [.ONNX](https://onnx.ai/) format.
- **Accurate Translations and Pronunciations**: Providing accurate translations and helpful phonetics was crucial for the learning experience wich needed proper research and validation.

## Learnings

This project was a fantastic opportunity to delve into ML and AR. I learned how to train ML models, integrate them into AR experiences, and optimize them for real-time performance. It also reinforced the importance of user-centric design in educational tools.

## Future Enhancements

- **Expanded Vocabulary**: Adding more objects and words to enrich the learning experience.
- **Audio Pronunciations**: Including audio to help users hear the correct pronunciation.
- **Interactive Learning Features**: Implementing quizzes or games to reinforce learning.

## Conclusion

Working on the **Learn Languages** project allowed me to combine my interest in AR. By creating an interactive AR experience, I aimed to make learning new languages more engaging and accessible. I hope this tool inspires others to explore new languages in a fun and immersive way and also inspires developers to leverage AR and ML for educational purposes in other domains.